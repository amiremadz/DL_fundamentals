{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP for NLP\n",
    "------\n",
    "\n",
    "Train and evaluate a simple MLP on the Reuters newswire topic classification task.\n",
    "\n",
    "Dataset of 11,228 newswires from Reuters, labeled over 46 topics. \n",
    "\n",
    "As with the IMDB dataset, each wire is encoded as a sequence of word indexes (same conventions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "8982 train sequences\n",
      "2246 test sequences\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "max_words = 1000\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing sequence data...\n",
      "x_train shape: (8982, 1000)\n",
      "x_test shape: (2246, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizing sequence data...')\n",
    "max_words = 1000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 classes\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(None) # TODO: Add Dense layer. How big should input_shape being?\n",
    "model.add(None) # TODO: Add Activation\n",
    "model.add(None) # TODO: Add Dense layer that matches output shape?\n",
    "model.add(None) # TODO: Add Activation to \"squashes\" the activation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/5\n",
      "8083/8083 [==============================] - 1s - loss: 2.4800 - acc: 0.4279 - val_loss: 1.9588 - val_acc: 0.5139\n",
      "Epoch 2/5\n",
      "8083/8083 [==============================] - 0s - loss: 1.8336 - acc: 0.5486 - val_loss: 1.7449 - val_acc: 0.5795\n",
      "Epoch 3/5\n",
      "8083/8083 [==============================] - 0s - loss: 1.6704 - acc: 0.5936 - val_loss: 1.6323 - val_acc: 0.6363\n",
      "Epoch 4/5\n",
      "8083/8083 [==============================] - 0s - loss: 1.5592 - acc: 0.6347 - val_loss: 1.5507 - val_acc: 0.6652\n",
      "Epoch 5/5\n",
      "8083/8083 [==============================] - 0s - loss: 1.4662 - acc: 0.6598 - val_loss: 1.4827 - val_acc: 0.6819\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "\n",
    "model.compile(loss=None, # TODO: What should the loss function be?\n",
    "              optimizer='SGD',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=True,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1312/2246 [================>.............] - ETA: 0sTest score: 1.46\n",
      "Test accuracy: 0.676\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size, \n",
    "                                verbose=True)\n",
    "\n",
    "print('Test score: {:.3}'.format(score))\n",
    "print('Test accuracy: {:.3}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__: What is baseline peformance? How does this simple model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__: Spend a little bit of time tuning hyperparameters. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
